## 1、GD(Gradient Descent, 梯度下降法)
梯度下降法是一种一阶优化算法，用于通过迭代最小化一个函数。常用于机器学习、深度学习、最小二乘拟合等。
在多维空间中，函数值沿着梯度的负方向下降最快，因此我们每一步都沿着负梯度方向更新变量，从而找到最小值。

优点：
- 只需要计算目标函数的梯度，不需要二阶导数或复杂矩阵分解
- 适用范围广，几乎所有可微分函数都能用梯度下降法优化，不局限于特定模型或函数形式
- 适合大规模问题，特别是批量（Batch）或小批量（Mini-batch）梯度下降，可以处理非常大规模的数据和参数
- 灵活性强，可以结合多种改进策略（动量、学习率衰减、自适应学习率如 Adam、RMSProp 等）提升效果

不足：
- 收敛速度慢，对于曲率不均匀的目标函数（比如条件数差异大），收敛效率较低，尤其是纯梯度下降
- 容易陷入局部极小/鞍点
- 依赖学习率设置，学习率过大可能导致不收敛或震荡，过小则收敛很慢；合适的学习率难调
- 对鞍点/平坦区域敏感，梯度小或者消失时，更新步长变得非常缓慢
- 不利用二阶信息，没有利用 Hessian 信息，不能充分利用目标函数曲率，效率不及牛顿类方法

梯度下降法种类：
- Gradient Descent: 每次使用全部样本计算梯度，稳定但慢
- Stochastic Gradient Descent (SGD): 每次使用一个样本，更新快但震荡大
- Mini-Batch Gradient Descent: 每次用小批量样本，兼顾速度与稳定性
- Momentum: 引入动量项，提高收敛速度，避免震荡
- Adagrad/RMSProp/Adam: 自适应调整学习率，更适用于非凸优化

### GD算法流程
已知：
- 目标函数 f(x)
- 梯度 ∇f(x)
- 初始值 x₀
- 学习率 η（步长）
- 最大迭代次数 或 收敛判据

#### 1. 线性近似（梯度方向一阶展开）
假设目标是最小化某个标量函数 `f(x)`，可以用泰勒一阶展开近似:

`f(x+Δx) ≈ f(x)+∇f(x)TΔx`

其中:
- ∇f(x)：函数在当前点的梯度（方向导数）
- Δx：变量的微小变化量

在x附近，我们可以用梯度方向来近似函数的变化趋势。
我们期望选择一个方向Δx，使得函数值下降最快,即负梯度方向:

`Δx = −η∇f(x) ⇒ xt+1 = xt -η∇f(xt)`

#### 2. 构建优化目标
选择最速下降方向，即负梯度方向:

`Δx = −η∇f(x)`

其中:
- η 是学习率（step size），控制每次前进的步长

因此更新公式为：

`xt+1 = xt −η∇f(xt)`

#### 3. 迭代更新参数

```
输入: 初始参数 x₀, 学习率 η, 最大迭代次数 T
输出: 最优解 x
for t = 0 to T:
    g = ∇f(xₜ)
    xₜ₊₁ = xₜ - η * g
```

#### 4. 收敛与终止条件
梯度下降会一直迭代，直到满足以下之一：
- 梯度范数足够小（接近平坦区域）：‖∇f(x)‖ < ε
- 目标函数变化极小：|f(xₜ₊₁) − f(xₜ)| < ε
- 达到最大迭代次数： t ≥ T_max