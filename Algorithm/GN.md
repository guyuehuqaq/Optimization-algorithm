# 1、GN(Gauss-Newton，高斯牛顿)
GN是用于求解非线性最小二乘问题的经典优化算法。

优点：
- 收敛速度快,通常比纯梯度下降快，尤其是当残差函数接近线性时，GN 方法表现很好
- 利用结构化信息,利用残差函数的雅可比矩阵J，GN 能够近似 Hessian 矩阵𝐽⊤𝐽,避免计算复杂的二阶导数
- 实现相对简单, 不需要计算目标函数的二阶导数，只需计算残差的一阶导数（雅可比矩阵）
- 适合大规模问题,对于参数维度适中、残差个数较多的非线性最小二乘问题，GN 方法高效实用

不足：
- 对初始值敏感，GN 是局部优化方法，初始参数如果离真实解较远，可能收敛到局部极小值，甚至不收敛
- Hessian 矩阵近似可能不准确，忽略了残差函数二阶导数项，非线性强时近似失效，收敛性变差
- 不保证全局收敛，当问题高度非线性或噪声大时，容易出现振荡或发散
- 矩阵𝐽⊤𝐽可能病态，导致数值不稳定，需要加正则化（如 Levenberg-Marquardt 算法）提高稳定性
- 无法处理非最小二乘问题，仅适合平方残差最小化，不能直接用于其它形式的目标函数

## 高斯-牛顿方法步骤

### 1. 线性化（泰勒展开）

对非线性残差函数 `f(x)` 一阶展开：

`f(x + Δx) ≈ f(x) + J(x) Δx`

其中：
- `J(x)` 是残差函数对参数的雅可比矩阵

---

### 2. 构建最小二乘问题

我们要最小化：

`minimize over Δx:  || f(x) + J(x) Δx ||^2`

展开并令导数为零，得到正规方程（normal equation）：

`J^T * J * Δx = - J^T * f`

---

### 3. 解更新量

`Δx = - (J^T * J)^(-1) * J^T * f`

---

### 4. 更新参数

`x ← x + Δx`