# Optimization-algorithm
### 记录了一些优化方法的实现和测试示例

## 1、DLT(Direct Linear Transform，直接线性变换)
DLT是一种基于线性代数的算法，将几何关系转化为线性代数问题，通过线性最小二乘求解参数，
本质是最小化线性误差的直接线性解。

优点：
- 直接、简单，数学清晰
- 能用最小奇异值直接求解
- 计算速度快

不足:
- 对数据噪声敏感
- 需要配合归一化（normalization）操作才能稳定
- 不能直接处理异常值，需要配合 RANSAC 等鲁棒方法

## 2、GN(Gauss-Newton，高斯牛顿)
GN是用于求解非线性最小二乘问题的经典优化算法。
### 高斯-牛顿方法步骤

#### 1. 线性化（泰勒展开）

对非线性残差函数 `f(x)` 一阶展开：

`f(x + Δx) ≈ f(x) + J(x) Δx`

其中：
- `J(x)` 是残差函数对参数的雅可比矩阵

---

#### 2. 构建最小二乘问题

我们要最小化：

`minimize over Δx:  || f(x) + J(x) Δx ||^2`

展开并令导数为零，得到正规方程（normal equation）：

`J^T * J * Δx = - J^T * f`

---

#### 3. 解更新量

`Δx = - (J^T * J)^(-1) * J^T * f`

---

#### 4. 更新参数

`x ← x + Δx`

## 3、GD(Gradient Descent, 梯度下降法)
梯度下降法是一种一阶优化算法，用于通过迭代最小化一个函数。常用于机器学习、深度学习、最小二乘拟合等。
在多维空间中，函数值沿着梯度的负方向下降最快，因此我们每一步都沿着负梯度方向更新变量，从而找到最小值。

梯度下降法种类：
- Gradient Descent: 每次使用全部样本计算梯度，稳定但慢
- Stochastic Gradient Descent (SGD): 每次使用一个样本，更新快但震荡大
- Mini-Batch Gradient Descent: 每次用小批量样本，兼顾速度与稳定性
- Momentum: 引入动量项，提高收敛速度，避免震荡
- Adagrad/RMSProp/Adam: 自适应调整学习率，更适用于非凸优化

### GD算法流程
已知：
- 目标函数 f(x)
- 梯度 ∇f(x)
- 初始值 x₀
- 学习率 η（步长）
- 最大迭代次数 或 收敛判据

#### 1. 线性近似（梯度方向一阶展开）
假设目标是最小化某个标量函数 `f(x)`，可以用泰勒一阶展开近似:

`f(x+Δx) ≈ f(x)+∇f(x)T Δx`

其中:
- ∇f(x)：函数在当前点的梯度（方向导数）
- Δx：变量的微小变化量

我们期望选择一个方向Δx，使得函数值下降最快。

#### 2. 构建优化目标
选择最速下降方向，即负梯度方向:

`Δx = −η∇f(x)`

其中:
- η 是学习率（step size），控制每次前进的步长

因此更新公式为：

`xt+1 = xt −η∇f(xt)`

#### 3. 迭代更新参数

```
输入: 初始参数 x₀, 学习率 η, 最大迭代次数 T
输出: 最优解 x
for t = 0 to T:
    g = ∇f(xₜ)
    xₜ₊₁ = xₜ - η * g
```

#### 4. 收敛与终止条件
梯度下降会一直迭代，直到满足以下之一：
- 梯度范数足够小（接近平坦区域）：‖∇f(x)‖ < ε
- 目标函数变化极小：|f(xₜ₊₁) − f(xₜ)| < ε
- 达到最大迭代次数： t ≥ T_max


