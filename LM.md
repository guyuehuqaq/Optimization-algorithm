# 1、LM(Levenberg-Marquardt)
LM算法是一种用于非线性最小二乘问题的优化算法，结合了以下两者的优点:
- **梯度下降法**: 在远离极小点时稳定，但收敛速度慢
- **高斯-牛顿法**: 在接近极小点时快速收敛，但对初值敏感

LM算法通过引入阻尼参数，平滑过渡两种方法，兼具稳定性和收敛速度。

主要区别就是在增量方程上增加了阻尼因子:

```
              (J^T J + λI) Δx = -J^T r

r = [r1, r2, ..., rN]^T
λ ≥ 0 是阻尼因子
当 λ=0 时, J^T J Δx = -J^T r , 近似退化为高斯-牛顿法
当 λ 很大时，λI Δx = -J^T r, 近似变成梯度下降法（沿梯度方向）
```

## 1. 问题定义
目标是求解

`min_x F(x) = 1/2 ∑_{i=1}^N r_i(x)^2`

其中：
```
x ∈ R^n ：待优化参数向量
r_i(x) : R^n → R ：第 i 个残差函数
```

## 2. 核心思想

在迭代的每一步，利用残差的一阶泰勒展开线性近似：

`r_i(x + Δx) ≈ r_i(x) + J_i Δx`

其中：
```
J_i = ∂r_i/∂x 是第 i 个残差对参数的雅可比（行向量）
组合成雅可比矩阵 J ∈ R^{N×n}
```

## 3. 算法流程
### 1. 初始化
- 设定初始参数 x0
- 选择初始阻尼系数 λ0（如小正数）
- 阻尼调整因子 ν > 1（一般取2或10）
### 2. 计算当前残差向量 r 和雅可比矩阵 J
### 3. 构造线性方程
`A = J^T J + λ I, b = -J^T r`
### 4. 求解线性方程 A Δx = b，得到参数增量 Δx
### 5. 更新参数尝试
`x_new = x + Δx`
### 6. 计算目标函数值
`F(x) = 1/2 ||r(x)||^2, F(x_new) = 1/2 ||r(x_new)||^2`
### 7. 判断是否接受更新
```
若 F(x_new) < F(x)，接受更新：
  x ← x_new, λ ← λ / ν
否则，拒绝更新，增大阻尼参数：
  λ ← λ × ν
```
### 8. 检查收敛条件
- ||Δx|| 足够小
- 目标函数变化足够小
- 达到最大迭代次数
### 9. 循环迭代，直到满足收敛条件

### 伪代码流程
```
输入：初始参数 x0，初始阻尼 λ0，调整因子 ν > 1
设置 x = x0，λ = λ0
while not converged:
    计算残差向量 r 和雅可比矩阵 J
    计算 A = J^T J + λ I，b = -J^T r
    求解线性方程 A Δx = b
    计算目标函数值 F(x) 和尝试更新后 F(x + Δx)

    if F(x + Δx) < F(x):
        x ← x + Δx
        λ ← λ / ν  # 降低阻尼，更信任高斯牛顿步长
    else:
        λ ← λ * ν  # 增大阻尼，更信任梯度方向
        
    检查收敛条件
输出 最优参数 x
```

